{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel pool (parpool) using the 'local' profile ... connected to 4 workers.\n",
      "\n",
      "ans = \n",
      "\n",
      " Pool with properties: \n",
      "\n",
      "            Connected: true\n",
      "           NumWorkers: 4\n",
      "              Cluster: local\n",
      "        AttachedFiles: {}\n",
      "          IdleTimeout: 30 minute(s) (30 minutes remaining)\n",
      "          SpmdEnabled: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%clear all variables\n",
    "clear all;\n",
    "\n",
    "%reset the RNG to default\n",
    "rng('default');\n",
    "\n",
    "%start parallel processing\n",
    "parpool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: FMINUNC - The Problem of Local Optima\n",
    "\n",
    "Numerically minimize the function $\\sin(x) + 0.01 (x âˆ’ a)^{2}$ , setting a = 0, using Matlab's fminunc.\n",
    "\n",
    "First, create an anonymous function handle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%set the constant, a:\n",
    "a=0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set the anonymous function to maximize\n",
    "f = @(x) sin(x)+0.01*(x-a).^2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the function over the interval $\\left(-5\\pi, 5\\pi \\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGkCAIAAACgjIjwAAAACXBIWXMAABcSAAAXEgFnn9JSAAAA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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fplot(f,[-5*pi,5*pi]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function has many local minima. The challenge will be finding the global minima (which seems to be between -5 and 0. \n",
    "\n",
    "Let's start to set the maximization routine by setting up the options that we will need (the alogrithm we want to use, the starting value, etc.). \n",
    "\n",
    "First, we will use the algorithm 'quasi-Newton' method. Then, we will provide the gradient (since this is an analytic problem) to use a better alogrithm (I will make use of python's symbolic computer algebra system to find the gradient - the first direvative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set the options\n",
    "%1. The algorithm will be quasi-newton (this is because the \n",
    "%   default algorithm, 'trust-region', requires the user to\n",
    "%   input the gradient of the function as a second function),\n",
    "%2. We will turn the display off (so we don't have a bunch of\n",
    "%   crap on the screen).\n",
    "%3. We will ask that the hessian use the 'bfsg' algorithm to \n",
    "%   update the search pattern.\n",
    "\n",
    "options = optimoptions(@fminunc,'Algorithm','quasi-newton',...\n",
    "                       'Display','off','HessUpdate','bfgs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_val =\n",
      "\n",
      "    4.6199\n",
      "\n",
      "\n",
      "f_val =\n",
      "\n",
      "   -0.7823\n"
     ]
    }
   ],
   "source": [
    "%set the starting value for guessing (a random integer between\n",
    "%-20 and 20. \n",
    "x0 = 2.5;\n",
    "\n",
    "%write the optimization routine\n",
    "[min_val,f_val] = fminunc(f,x0,options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) and (b): The above code finds the minimum, however it is dependent on the starting value. Different starting values outside the domain $x\\in [-4.8,1.6]$ produce local minimas. Because in the above our starting value was outside this domain, we obtain a local minima\n",
    "\n",
    "## Finding the gradient and hessian with Symbolic Python\n",
    "\n",
    "Let's find the gradient (in our case, with only one variable, the first direvative) so that we may see if this removes sensitivity. We'll use Python's sympy to do symbolic mathematics (we use cell magic '%%python' to switch us to python for just these cells). We will provide these as additional functions as practice to learn how to provide these to an optimization routine if they are needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "\n",
    "#Import sympy\n",
    "from sympy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.02*a + 0.02*x + cos(x)\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "\n",
    "#declare symbols\n",
    "x,a = symbols('x a')\n",
    "\n",
    "#differentiate our objective function\n",
    "print diff(sin(x)+0.01*(x-a)**2,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-sin(x) + 0.02\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "\n",
    "#differentiate again for hessian\n",
    "print diff(sin(x)+0.01*(x-a)**2,x,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now program the gradient to provide it to fminunc. We will add them to the anonymous function to do this directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%program the gradient and hessian as anon functions\n",
    "%we use the deal method to have it deal out the \n",
    "%different functions\n",
    "\n",
    "f_grad_hess = @(x) deal(sin(x)+0.01*(x-a).^2,...\n",
    "                        -0.02*a + 0.02*x + cos(x),...\n",
    "                        -sin(x) + 0.02);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%create a new options to tell matlab we will supply gradient\n",
    "options_2 = optimoptions('fminunc','Algorithm','trust-region',...\n",
    "                         'GradObj','on','Hessian','on',...\n",
    "                         'Display','off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_val_2 =\n",
      "\n",
      "    4.6199\n",
      "\n",
      "\n",
      "f_val_2 =\n",
      "\n",
      "   -0.7823\n"
     ]
    }
   ],
   "source": [
    "%write the new minimization routine\n",
    "\n",
    "[min_val_2,f_val_2] = fminunc(f_grad_hess,x0,options_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Use Multistart or Simulated Annealing for Global Optimization\n",
    "\n",
    "Providing the gradient and the hessian does not help us here. It has no impact. Perhaps we can create a global optimization search in matlab to help find the minimum regardless of the starting value?\n",
    "\n",
    "Because we are using 'fminunc' we will have to use Multistart. We can also parallelize Multistart in case the objective function happens to create slow computation. Multistart will use the otimization toolbox methods to tell matlab to: 1) create a problem using the desired solver and the input parameters of the problem; 2) declare that we are going to use a multistart (and optionally, if we want to run in parallel); 3) run the multistart problem on as many local solvers as we wish (it will automatically generate a vector of starting points at what are known as 'basins of attraction', or \"areas\" in the function that have steep descent/acent and thus where an optima may exist). The solvers are distributed on the cores of your computer. \n",
    "\n",
    "So basically, for part (c) of the problem this (along with the next section) is a strategy to use. For any $a\\in(-\\pi,\\pi)$ we can find global minima with this algorithm of distributed starting values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%number of solvers:\n",
    "n_solvers=50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%Since we have already defined the problems previously, lets recycle\n",
    "%(we could use the original f function that had matlab compute approx.\n",
    "%hessians and numeric gradients, but since we have easy analytical \n",
    "%solutions we can just use the f_grad_hess all the same).\n",
    "\n",
    "%with the original problem in the first cells above, define problem\n",
    "problem = createOptimProblem('fminunc','objective',f_grad_hess,...\n",
    "                             'x0',x0,'options', options_2);\n",
    "                             \n",
    "ms = MultiStart('UseParallel', true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiStart completed the runs from all start points.\n",
      "\n",
      "All 50 local solver runs converged with a positive local solver exit flag.\n",
      "\n",
      "min_val_3 =\n",
      "\n",
      "   -1.5400\n",
      "\n",
      "\n",
      "f_val_3 =\n",
      "\n",
      "   -0.9758\n"
     ]
    }
   ],
   "source": [
    "%write the MultiStart routine on 50 points\n",
    "\n",
    "[min_val_3,f_val_3] = run(ms,problem,n_solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm converged on the global optima. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Simulated Annealing to Globally Optimize\n",
    "\n",
    "Here we will use simulated annealing, a method to globally optimize that does not require derivatives to conduct its optimization routine. So we will use the function $f$ we defined in the first part of the problem with no gradient or hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%Let's set the options for simulated annealing, we want display off\n",
    "options_3 = saoptimset('Display','off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_val_4 =\n",
      "\n",
      "   -1.5401\n",
      "\n",
      "\n",
      "f_val_4 =\n",
      "\n",
      "   -0.9758\n"
     ]
    }
   ],
   "source": [
    "%Write the maximization routine\n",
    "\n",
    "%we will reuse the initial starting point from the Multistart\n",
    "%problem, no need to create a new one.\n",
    "\n",
    "[min_val_4,f_val_4] = simulannealbnd(f,x0,[],[],options_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the algorithm also is able to converge to the global optima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: FMINCON\n",
    "\n",
    "We will resolve the problem with the constraint that $x\\in[0,3]$ using fmincon. We can either supply the gradient and the hessian or we can leave it out (that is, we can choose to use the f function or the f_grad_hess function that contains the gradient and the hessian). The default algorithm does not use user-supplied gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set the options for fmincon, where we provide\n",
    "%user-supplied gradients and hessians:\n",
    "\n",
    "options_4 = optimoptions('fmincon','Algorithm','trust-region-reflective',...\n",
    "                         'GradObj','on','Hessian','on','Display','off',...\n",
    "                         'UseParallel',true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_val_5 =\n",
      "\n",
      "    3.0000\n",
      "\n",
      "\n",
      "f_val_5 =\n",
      "\n",
      "    0.2311\n"
     ]
    }
   ],
   "source": [
    "%the upper and lower bounds:\n",
    "xlb = 0;\n",
    "xub = 3;\n",
    "\n",
    "%write the constrained minimization routine:\n",
    "\n",
    "[min_val_5,f_val_5] = fmincon(f_grad_hess,x0,[],[],...\n",
    "                              [],[],xlb,xub,[],options_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above minimization is likewise sensitive to starting values because both of the boundries, 0 and 3, are local minima. This can be seen in the graph, as there is a local maxima in $x\\in[0,3]$ that creates differente basins of attraction. We know that the true answer is 0. So we can also implement the global search routine to obtain this minima. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Using Mutistart or Simulated Annealing for Global Optimization\n",
    "\n",
    "We can also use MultiStart to solve for the global optima of our constraint problem. Because we are using FMINCON, we can use matlab's GlobalSearch problem instead of MultiSearch (however, if we want to parallelize the problem across our computer's cores we would have to use MultiStart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%Since we have already defined the problems previously, lets recycle\n",
    "%We will use the associated optimization options and the objective\n",
    "%function (with the programmed hessian and gradient) from f_grad_hess\n",
    "%(we could also use f, but would need to adjust options to turn off\n",
    "%the user-supplied gradients and hessian).\n",
    "\n",
    "%with the original problem in the first cells above, define problem\n",
    "problem_2 = createOptimProblem('fmincon','objective',f_grad_hess,...\n",
    "                               'x0',x0,'lb',xlb,'ub',xub,...\n",
    "                               'options', options_4);\n",
    "                             \n",
    "ms_2 = MultiStart('UseParallel', true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiStart completed the runs from all start points.\n",
      "\n",
      "All 50 local solver runs converged with a positive local solver exit flag.\n",
      "\n",
      "min_val_6 =\n",
      "\n",
      "   7.0460e-12\n",
      "\n",
      "\n",
      "f_val_6 =\n",
      "\n",
      "   7.0460e-12\n"
     ]
    }
   ],
   "source": [
    "%write the MultiStart routine on 100 points\n",
    "\n",
    "[min_val_6,f_val_6] = run(ms_2,problem_2,n_solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globally Constrained Optimization with Simulated Annealing\n",
    "\n",
    "We can also use simulated annealing here with matlab. But ONLY because our constraints are bounds (S.A. in matlab currently only works with constraints that form upper and lower bounds; if we had linear euqality, inequality, or nonlinear constraints we would have to use FMINCON because S.A. does not allow for these types of constraints). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_val_7 =\n",
      "\n",
      "   8.5550e-10\n",
      "\n",
      "\n",
      "f_val_7 =\n",
      "\n",
      "   8.5550e-10\n"
     ]
    }
   ],
   "source": [
    "%We can recycle code from problem 1 and the bounds already \n",
    "%defined. We will not need to create a new set of options either.\n",
    "%Once again, we will use the function f we defined previously and\n",
    "%not the function with the gradient and the hessian. We will also \n",
    "\n",
    "[min_val_7,f_val_7] = simulannealbnd(f,x0,xlb,xub,options_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: OLS minimization with FMINUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%Load the data\n",
    "data = load('NerloveData.m');\n",
    "\n",
    "%Remove the first column ('names' of firms):\n",
    "data(:,1)=[];\n",
    "\n",
    "%Sample Size (number of rows = number of samples)\n",
    "N = size(data,1);\n",
    "\n",
    "%Create the Y and X arrays (remember to take the logs):\n",
    "\n",
    "%Y is column 1\n",
    "Y = log(data(:,1));\n",
    "\n",
    "%X matrix will be from data(:,2:end) and a constant\n",
    "constant = ones(N,1);\n",
    "\n",
    "X = [constant log(data(:,2:end))];\n",
    "\n",
    "%Number of regressors:\n",
    "K = size(X,2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%Create the anonymous function that will be minimized:\n",
    "\n",
    "SSR = @(beta) (Y - X*beta)'*(Y - X*beta);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%Create the initial value of the betas (we have 5). We\n",
    "%will just start with a vecotr of 0s.\n",
    "\n",
    "beta_0 = zeros(K,1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_hat =\n",
      "\n",
      "   -3.5347\n",
      "    0.7204\n",
      "    0.4371\n",
      "    0.4264\n",
      "   -0.2184\n",
      "\n",
      "\n",
      "SSR_min =\n",
      "\n",
      "   21.5520\n"
     ]
    }
   ],
   "source": [
    "%Create the minimization routine:\n",
    "%we can use the same options we used for our function f \n",
    "%in problem 1 since we use the same routine.\n",
    "\n",
    "[beta_hat,SSR_min] = fminunc(SSR,beta_0,options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma_hat =\n",
      "\n",
      "    0.3924\n",
      "\n",
      "\n",
      "SE_beta =\n",
      "\n",
      "    1.7744\n",
      "    0.0175\n",
      "    0.2910\n",
      "    0.1004\n",
      "    0.3394\n"
     ]
    }
   ],
   "source": [
    "%Generate the epsilons and the SE of the betas\n",
    "eps = Y-X*beta_hat;\n",
    "\n",
    "%model variance:\n",
    "var = eps'*eps/(N-K);\n",
    "\n",
    "%the standard deviation of the model:\n",
    "sigma_hat = sqrt(var)\n",
    "\n",
    "%Variance-Covariance matrix of the beta parameters:\n",
    "Var_Cov_beta = var*inv(X'*X);\n",
    "\n",
    "%The standard errors of the betas:\n",
    "SE_beta = sqrt(diag(Var_Cov_beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel pool using the 'local' profile is shutting down.\n"
     ]
    }
   ],
   "source": [
    "%shut down parallel computing\n",
    "delete(gcp('nocreate'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Matlab",
   "language": "matlab",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-matlab",
   "name": "matlab",
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
